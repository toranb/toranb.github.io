---
layout: post
title: "Building BM25 Keyword Search In Postgres"
date:   2025-08-16 01:00:00
categories: blog archive
---
<p>It turns out, you can build a surprisingly powerful keyword search, known as BM25, with just a handful of PSQL functions. What follows is my journey including what I've learned, how this search algorithm works in simple terms, and how I implemented it from scratch.</p>

<h3>The Foundation: From TF-IDF to BM25</h3>

<p>Before diving into BM25, it's helpful to understand its predecessor, TF-IDF (Term Frequency-Inverse Document Frequency). It's a simple but powerful idea that helps determine a word's importance to a document within a collection of documents.</p>

<ul>
  <li><strong>Term Frequency (TF):</strong> This is just what it sounds like. How often does a term appear in a single document? The intuition is that if the word "data" appears 5 times in a 100-word article, it's probably more relevant to that article than a word that appears only once.</li>
  <li><strong>Inverse Document Frequency (IDF):</strong> This measures how common or rare a word is across *all* documents. Common words like "the" or "a" will appear everywhere and have a low IDF score, effectively penalizing them. Rare words will appear in fewer documents and thus have a higher IDF score, signaling that they are more significant.</li>
</ul>

<p>By multiplying these two scores, you get the TF-IDF score, which gives you a weighted value for how important a word is to a specific document. This was the starting point for my exploration.</p>

<p>BM25 is essentially a more refined version of this. It builds on the core concepts of TF-IDF but adds a couple of clever improvements:</p>

<ul>
  <li><strong>Term Frequency Saturation:</strong> BM25 recognizes that a word's relevance doesn't increase linearly. The difference between a word appearing zero times and one time is huge. The difference between it appearing 20 times and 21 times is negligible. It uses a parameter k1 to control how quickly the term frequency score "saturates."</li>
  <li><strong>Document Length Normalization:</strong> Longer documents naturally have an advantage because they have more opportunities for a search term to appear. BM25 penalizes longer documents to level the playing field, ensuring that relevance isn't just a side effect of verbosity.</li>
</ul>

<h3>The Implementation: A PostgreSQL Deep Dive</h3>

<p>The beauty of this approach is that the entire search algorithm lives within PostgreSQL and can be called from any application.</p>

<p>Here's a breakdown of the core components:</p>

<h4>1. Text Processing and Tokenization</h4>

<p>The first step in any search system is to break down raw text into meaningful terms, or tokens. I created a tokenize_and_count function to handle this. It takes a block of text and converts it to lowercase, removes common stop words using PostgreSQL's built-in English stop words list (like 'a', 'the', 'is'), uses stemming to reduce words to their root form (e.g., "canceling" and "canceled" both become "cancel"), filters out blank tokens, and finally returns a table of unique terms and their frequencies in the text.</p>

<div class="highlight" data-language="elixir">
  <pre class="language-elixir">
    <code class="language-elixir">CREATE OR REPLACE FUNCTION tokenize_and_count(input_text TEXT)</code>
    <code class="language-elixir">RETURNS TABLE (term TEXT, count INTEGER) AS $$</code>
    <code class="language-elixir">BEGIN</code>
    <code class="language-elixir">    -- Get stemmed tokens with stop words removed</code>
    <code class="language-elixir">    RETURN QUERY</code>
    <code class="language-elixir">    WITH tokens AS (</code>
    <code class="language-elixir">        SELECT word</code>
    <code class="language-elixir">        FROM ts_parse('default', lower(input_text)) AS t(tokid, word)</code>
    <code class="language-elixir">        WHERE tokid != 12  -- Filter out blank tokens</code>
    <code class="language-elixir">    ),</code>
    <code class="language-elixir">    processed_tokens AS (</code>
    <code class="language-elixir">        SELECT</code>
    <code class="language-elixir">            CASE</code>
    <code class="language-elixir">                WHEN ts_lexize('public.simple_dict', word) = '{}'</code>
    <code class="language-elixir">                THEN NULL  -- It's a stop word</code>
    <code class="language-elixir">                ELSE COALESCE(</code>
    <code class="language-elixir">                    (ts_lexize('public.english_stem', word))[1],</code>
    <code class="language-elixir">                    word</code>
    <code class="language-elixir">                )</code>
    <code class="language-elixir">            END AS processed_word</code>
    <code class="language-elixir">        FROM tokens</code>
    <code class="language-elixir">    )</code>
    <code class="language-elixir">    SELECT</code>
    <code class="language-elixir">        processed_word as term,</code>
    <code class="language-elixir">        COUNT(*)::INTEGER</code>
    <code class="language-elixir">    FROM processed_tokens</code>
    <code class="language-elixir">    WHERE processed_word IS NOT NULL</code>
    <code class="language-elixir">    GROUP BY processed_word;</code>
    <code class="language-elixir">END;</code>
    <code class="language-elixir">$$ LANGUAGE plpgsql;</code>
  </pre>
</div>

<h4>2. Storing Statistics for Performance</h4>

<p>Calculating these scores on the fly for every document during every search would be incredibly slow. To optimize this, I created two key tables:</p>

<ul>
  <li><code>verse_stats</code>: This table stores the pre-calculated term frequencies for each document. It holds the document's ID, its total length, and a JSONB object mapping each term to its count.</li>
  <li><code>term_stats</code>: This table stores global statistics about each term, such as how many documents it appears in doc_count. This is crucial for calculating the IDF score efficiently.</li>
</ul>

<p>I also created a materialized view, global_stats, to keep track of the total number of documents and the average document length across the entire corpus, which is needed for the BM25 calculation. It's important to note that this materialized view needs to be refreshed whenever documents are added or updated to ensure accurate statistics. The migration includes the helper function update_modified_verses that handle this refresh automatically.</p>

<h4>3. The BM25 Scoring Functions</h4>

<p>With the data structures in place, I implemented the core BM25 logic. The <code>calculate_idf</code> function uses the BM25 formula to determine a term's weight. It's slightly more complex than the standard TF-IDF version but ensures scores are non-negative.</p>

<div class="highlight" data-language="elixir">
  <pre class="language-elixir">
    <code class="language-elixir">CREATE OR REPLACE FUNCTION calculate_idf(term_doc_count INTEGER, total_docs INTEGER)</code>
    <code class="language-elixir">RETURNS FLOAT AS $$</code>
    <code class="language-elixir">BEGIN</code>
    <code class="language-elixir">  -- Modified BM25 IDF formula to ensure non-negative scores</code>
    <code class="language-elixir">  RETURN ln(1 + (total_docs - term_doc_count + 0.5) /</code>
    <code class="language-elixir">                (term_doc_count + 0.5));</code>
    <code class="language-elixir">END;</code>
    <code class="language-elixir">$$ LANGUAGE plpgsql;</code>
  </pre>
</div>

<p>The bm25_term_score function brings it all together, combining the term frequency, document length, and IDF score to produce the final score for a single term in a single document.</p>

<h4>4. Putting it all Together: The Search Function</h4>

<p>The final piece is the search_verses function. This function orchestrates the entire process: it takes a user's query text, tokenizes it, and to handle typos, it uses the pg_trgm extension to find terms in term_stats that are similar to the (potentially misspelled) query terms. It then joins the query terms against the verse_stats and term_stats tables, calculates the bm25_term_score for each matching term in each document, sums them up to get a final relevance score, and returns the top results.</p>

<p>One important setup detail: the migration needs to enable the pg_trgm extension for the typo tolerance to work. This should be added at the beginning of your migration:</p>

<div class="highlight" data-language="elixir">
  <pre class="language-elixir">
    <code class="language-elixir">CREATE EXTENSION IF NOT EXISTS pg_trgm</code>
  </pre>
</div>

<p>The result is a fast, efficient, and typo-tolerant search engine built without any external dependencies. It was a fascinating journey into the mechanics of search, and it's incredibly satisfying to see it working with just a bit of clever SQL. Hopefully, this sheds some light on how modern keyword search works and inspires you to see what you can build with the tools you already have.</p>
