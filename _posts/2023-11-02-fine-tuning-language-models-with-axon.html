---
layout: post
title: "Fine tuning language models with Axon"
date:   2023-11-02 01:00:00
categories: blog archive
---

<p>I had the opportunity to speak at <a href="https://2023.elixirconf.com/">ElixirConf US</a> in Orlando and couldn't be happier with the outcome. My talk, initially designed as a high-level overview of <a href="https://hexdocs.pm/bumblebee/fine_tuning.html">fine-tuning with Axon</a>, quickly evolved into an introduction to machine learning tailored for software engineers. Below is a brief summary of the talk for those who are interested.

<h2>Summary</h2>

<p>The talk by Toran Billups is about his unique journey into understanding and implementing fine-tuning of language models like BERT. The talk aims to demystify the concepts of neural networks, deep learning, and the fine-tuning process for those who might find these topics intimidating or overly technical.</p>

<p>He uses the FizzBuzz problem as an introductory example to explain basic concepts in machine learning and neural networks. This leads to discussions on input and output data, weights, neural network fundamentals, and vector mathematics. He emphasizes the simplicity at the heart of neural networks—multiplication of inputs by weights to make predictions—and introduces concepts like weighted sum (dot product) and the softmax function for normalizing output probabilities.</p>

<p>He demonstrates how Axon simplifies building neural networks by abstracting much of the complexity involved in the process. He briefly covers topics like activation functions (ReLU and softmax), loss functions, and optimizers without diving too deeply into the mathematics, making the content accessible to beginners.</p>

<p>Toran then introduces the concept of fine-tuning with a focus on text classification, using a pre-trained model (BERT) as a starting point. He explains the process of creating a labeled dataset, transforming text into a form that the model can understand (tokenization and embedding), and training the model for a specific task. The talk provides insights into the importance of quality data, the challenges of data preparation, and the nuances of encoding text for machine learning models.</p>

<p>Throughout the talk, Toran encourages exploration and learning by sharing his code and examples on GitHub, emphasizing that the goal is to make deep learning and model fine-tuning more approachable and understandable for Elixir developers and those new to machine learning.</p>

<p>Toran concludes with reflections on the future of machine learning solutions, the shift towards off-the-shelf, industry-specific models, and the opportunities for developers to extend their skills into machine learning with Elixir. He recommends <a href="https://www.manning.com/books/grokking-deep-learning">Grokking Deep Learning</a> as an excellent resource for those interested in further exploring the topic.</p>

<div style="padding-top: 20px; padding-bottom: 20px;">
  <div style="margin: auto; width: 90vw; height: 50vw; max-width: 768px; max-height: 432px">
    <iframe width="100%" height="100%" src="https://www.youtube.com/embed/-iZIZHgHa5M?si=QUUNTUtgAVDdfPfz" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div>
</div>

<p>The source code is up on <a href="https://github.com/toranb/elixirconf2023">Github</a> for anyone who wanted to see a few of those examples in more detail. The full transcript for the talk can be downloaded <a href="/content/presentations/2024/finetune.txt">here</a>.</p>
